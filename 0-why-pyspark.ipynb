{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occupational-bibliography",
   "metadata": {},
   "source": [
    "# Why Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-pepper",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-escape",
   "metadata": {},
   "source": [
    "If we think about our normal understanding of databases, our databases have two components: (1) storage and (2) processing.  That is, we both store our data in our databases, and we can perform operations on that data.  \n",
    "\n",
    "As we'll see with Pyspark, we'll only be concerned with processing our data.  And we can be saved to disk externally, on a service like S3.  This has vast implications for the way we treat and value data in an organization, as well as the operations we can perform on that data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-exchange",
   "metadata": {},
   "source": [
    "### Databases: Storage and Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-advice",
   "metadata": {},
   "source": [
    "Now normally, when we work with a database, we are working with two components: storage and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-bristol",
   "metadata": {},
   "source": [
    "<img src=\"./db_both.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-voltage",
   "metadata": {},
   "source": [
    "We store our data in tables, and this data is stored in a file.  And when we perform an operation, like select records from that table, we load data into memory and perform any transformations (like rounding floats) and return data to the user.\n",
    "\n",
    "Even when working with a distributed database like redshift, where data is stored across compute nodes, each compute node only processes it's own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-resource",
   "metadata": {},
   "source": [
    "<img src=\"./distributed_db.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-strength",
   "metadata": {},
   "source": [
    "> So above, slice 1 both stores a partition of the movies, and calculate the average of that subset of movies.  Slice 2 does the same for it's subset of movies, and so on.  Then the leader node uses the sub-averages to calculate the average across all movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-category",
   "metadata": {},
   "source": [
    "### Pyspark stores data in memory\n",
    "\n",
    "As we may have guessed, things operate differently in pyspark.  Take a look at the diagram below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-slovak",
   "metadata": {},
   "source": [
    "> <img src=\"./s3_to_movies.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-amateur",
   "metadata": {},
   "source": [
    "Instead of storing the data on disk inside the database, instead we can permanently store the data externally in something like an S3 bucket, and then can read the data into Spark's memory when we need to perform a query on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-culture",
   "metadata": {},
   "source": [
    "So in the diagram above, notice that inside of spark, we do not need to store our data on disk.  Instead, inside of spark, our data can be directly loaded into memory.  \n",
    "\n",
    "And because Spark is organized as a cluster spark can also read our data into memory, partitioning it onto different machines in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-characterization",
   "metadata": {},
   "source": [
    "<img src=\"./spark_cluster_partition.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-contemporary",
   "metadata": {},
   "source": [
    "### But there's still local storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-musical",
   "metadata": {},
   "source": [
    "Even though Spark performs much of it's computation in memory, and even allows for in memory storage, Spark nodes still use local disks for data that does not fit into RAM, and to store intermediate output in a complex operation.\n",
    "\n",
    "So let's update our diagram to more accurately reflect the hardware spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-circular",
   "metadata": {},
   "source": [
    "> <img src='./spark_cluster_disk.jpg' width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-ownership",
   "metadata": {},
   "source": [
    "Still even though there is technically in memory storage in worker nodes, what distinguishes Spark is it's reliance on in memory storage and computation.\n",
    "\n",
    "Ok, now let's see some of the benefits on the reliance of in memory storage and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-diploma",
   "metadata": {},
   "source": [
    "### The Benefits of In Memory Storage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-pregnancy",
   "metadata": {},
   "source": [
    "So we saw that with Spark, we read in our data from an external data source, and in spark, we can store and process this data in memory.  Let's see some of the benefits that we get with this kind of storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-serum",
   "metadata": {},
   "source": [
    "1. Cheaper storage\n",
    "\n",
    "The first benefit is simply that it costs less money if we store our data externally on something like S3 than storing our data inside of a database on disk.  With our data stored on S3, our data is simply written to disk, and there is no associated software running along with it.  By contrast, when we store our data in a database like redshift or spark, for each node we have running, we are not just paying for the data storage but a database with the capability to process that data.\n",
    "\n",
    "With our data being cheaper to store, we can store data that has relatively low value, or unknown value.  And if a data scientist or data engineer can extract value from it later on, the data will be available.\n",
    "\n",
    "2. Schema on Read\n",
    "\n",
    "Not only is the data available, but with storage in memory, it's easier to explore and process this data.  For example, we can read data into a spark dataframe, which operates similarly to a pandas dataframe.  How easy is it to go from a csv file to a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "directed-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>imdb</th>\n",
       "      <th>title</th>\n",
       "      <th>test</th>\n",
       "      <th>clean_test</th>\n",
       "      <th>binary</th>\n",
       "      <th>budget</th>\n",
       "      <th>domgross</th>\n",
       "      <th>intgross</th>\n",
       "      <th>code</th>\n",
       "      <th>budget_2013$</th>\n",
       "      <th>domgross_2013$</th>\n",
       "      <th>intgross_2013$</th>\n",
       "      <th>period code</th>\n",
       "      <th>decade code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>tt1711425</td>\n",
       "      <td>21 &amp;amp; Over</td>\n",
       "      <td>notalk</td>\n",
       "      <td>notalk</td>\n",
       "      <td>FAIL</td>\n",
       "      <td>13000000</td>\n",
       "      <td>25682380.0</td>\n",
       "      <td>42195766.0</td>\n",
       "      <td>2013FAIL</td>\n",
       "      <td>13000000</td>\n",
       "      <td>25682380.0</td>\n",
       "      <td>42195766.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>tt1343727</td>\n",
       "      <td>Dredd 3D</td>\n",
       "      <td>ok-disagree</td>\n",
       "      <td>ok</td>\n",
       "      <td>PASS</td>\n",
       "      <td>45000000</td>\n",
       "      <td>13414714.0</td>\n",
       "      <td>40868994.0</td>\n",
       "      <td>2012PASS</td>\n",
       "      <td>45658735</td>\n",
       "      <td>13611086.0</td>\n",
       "      <td>41467257.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year       imdb          title         test clean_test binary    budget  \\\n",
       "0  2013  tt1711425  21 &amp; Over       notalk     notalk   FAIL  13000000   \n",
       "1  2012  tt1343727       Dredd 3D  ok-disagree         ok   PASS  45000000   \n",
       "\n",
       "     domgross    intgross      code  budget_2013$  domgross_2013$  \\\n",
       "0  25682380.0  42195766.0  2013FAIL      13000000      25682380.0   \n",
       "1  13414714.0  40868994.0  2012PASS      45658735      13611086.0   \n",
       "\n",
       "   intgross_2013$  period code  decade code  \n",
       "0      42195766.0          1.0          1.0  \n",
       "1      41467257.0          1.0          1.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/bechdel/movies.csv'\n",
    "df = pd.read_csv(url)\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-narrative",
   "metadata": {},
   "source": [
    "Pretty easy.  Contrast this with our operations with redshift or postgres.  There, we first needed to create our tables, and transform our data to insert records into those tables.  With Spark, we can let the software determine the schema that fits to the data.  This is called `schema on read`, as the schema is determined when we read in the data.\n",
    "\n",
    "This is also referred to as extract-load-transform (ELT), as we can load our external data into our spark database, and then transform this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-trigger",
   "metadata": {},
   "source": [
    "3. Memory intensive computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-encoding",
   "metadata": {},
   "source": [
    "The third benefit of memory storage really comes from the perspective of data science.  Performing certain computations requires having a large amount of data accessible for computation, and having our entire dataset loaded in memory means the data will be accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-spiritual",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-accreditation",
   "metadata": {},
   "source": [
    "In this lesson, we saw that with Pyspark we store our data externally in something like an S3 bucket, and then load this data into memory when used by Spark.  If we need additional memory, we add more nodes to our Spark cluster.\n",
    "\n",
    "Because Spark is in memory storage, this means that we are able to store data with low value, or unknown value.  And reading our data into memory means that we do not need to first create a schema and then load our data into these tables.  In fact spark often can detect the appropriate datatypes for each column for us, creating the `schema on read`.  Finally, for data scientists, Spark is useful for memory intensive calculations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
